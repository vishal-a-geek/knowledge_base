#spark
A “computer” is a single machine sitting on a desk at home or at work. Single machines do not have enough power and resources to perform computations on huge amounts of information.

**A cluster,** or group, of computers, pools the resources of many machines together, giving us the ability to use all the cumulative resources as if they were a single computer.

Now, a group of machines alone is not powerful, we need a framework to coordinate work across them. Spark does just that, managing and coordinating the execution of tasks on data across a cluster of computers.
![[Pasted image 20250411144344.png]]
### Spark Applications 
Spark Applications consist of a **driver process** and a set of **executor processes.**
##### Driver Process: 
The driver process runs main() function, sits on a node in the cluster, and is responsible for three things
- Maintaining information about the Spark Application
- Responding to a user’s program or input
- Analysing, distributing, and scheduling work across the executors
##### Executor Processes:  
The executors are responsible for **actually carrying out the work** that the driver assigns them.
- Executing code assigned to it by the driver
- Reporting the state of the computation on that executor back to the driver node.

Spark, in addition to its cluster mode, also has a local mode. Which means that they can live on the same machine or different machines. In local mode, the driver and executors run (as threads) on individual computer instead of a cluster.

### Spark’s Language APIs 

Spark’s language APIs make it possible to run Spark code using various programming languages.
- Scala
- Java
- Python
- SQL
- R
![[Pasted image 20250411154158.png]]
Each language API maintains the same core concepts that we described earlier. There is a SparkSession object available to the user, which is the entrance point to running Spark code. When using Spark from Python or R, you don’t write explicit JVM instructions; instead, you write Python and R code that Spark translates into code that it then can run on the executor JVMs.

### The SparkSession
Spark Application is controlled through a driver process called the **SparkSession.**

The SparkSession instance is the way Spark executes user-defined manipulations across the cluster. There is a one-to-one correspondence between a SparkSession and a Spark Application.

In Scala and Python, the variable is available as spark when you start the console.
```zsh
$ pyspark
>>>spark
```
output:
```
  <pyspark.sql.session.SparkSession at 0x7efda4c1ccd0>
```

Example:
```python
 myRange = spark.range(1000).toDF("number")
```

We created a DataFrame with one column containing 1,000 rows with values from 0 to 999.
This range of numbers represents a distributed collection.
When run on a cluster, each part of this range of numbers exists on a different executor.