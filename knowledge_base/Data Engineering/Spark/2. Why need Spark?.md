#spark #big_data
## Big Data Problem
For most of the history so far, computers became faster every year with processor speed increase

This trend in hardware stopped in around 2005, due to hard limits on heat dissipation, hardware devs stopped making individual processors faster, and switched towards adding more parallel CPU cores all running at the same speed.

This change meant that suddenly applications needed to be modified to add parallelism in order to run faster, which set the stage for new programming models such as Apache Spark.

In this new world, software developed in past 50 years cannot automatically scale up, neither can traditional programming models for data processing applications, creating need for new programming models. It is this world that Apache Spark was built for.
