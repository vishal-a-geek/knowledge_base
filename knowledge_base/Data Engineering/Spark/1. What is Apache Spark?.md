#spark #data_engineering
Apache Spark is unified **computing engine** and a set of libraries for parallel data processing on computer clusters.
> **compute engine**: any system with processing power and storage capability

Large terabytes/petabytes of data cannot be processed by a single computer. This is the size of data big giants like Google process every single day.
Few kilobytes (kbs) or megabytes (mbs) of data can be processed by a single computer

 Apache Spark is a framework designed for processing such large size data in parallel

## Spark's Toolkit
- **Low-Level APIs**
	- RDDs ^1bf2c0
	- Distributed Variables
- **Structured APIs** (used 80-90% in real world) ^e02090
	- Datasets
	- Dataframes
	- SQL
- **Structured Streaming**
- **Advanced Analytics**
- **Libraries & Ecosystem**

## Spark is Unified
Spark is designed to support a wide range of data analytics tasks
- Simple data loading
- SQL queries
- Machine Learning
- Streaming computation
All these are performed over the **same computing engine** and with a consistent set of APIs
- **consistent** **composable** **APIs** to build applications out of smaller pieces or out of existing libraries.
- **high performance** by optimising across different libraries and functions composed together
	- Spark automatically optimises based on it's internal libraries which are used in the application
	- **Spark Graph View** helps us understand the design of how the application is to be executed
- e.g. 
	- the application:
		- load data using SQL query
		- evaluate ML model using Spark's ML library
	- the spark engine would combine these steps into one scan over the data
	- no matter how you combine it's general APIs

## Spark is Unified Compute Engine
- Handles **loading** data from anywhere, any storage systems, including
	- AWS S3 #aws_s3 
	- Azure Storage
	- **Key-value** storage such as Apache Cassandra
	- **Message buses** such as Apache Kafka
- performs computation on the loaded data

- Data is expensive to move
	- So spark focuses on **performing computations on the data**
		- no matter where the data resides
		- without moving the data somewhere
			- Moving data is expensive
			- it computes in the memory (RAM) where the data resides
- Hadoop is also distributed data processing engine, popular before Spark
	- Hadoop has it's own file system HDFS
	- Hadoop has it's own compute system MapReduce
	- Problem with Hadoop:
		- Data must be stored in HDFS in order to work with MapReduce
		- Tight coupling between HDFS and MapReduce
## Spark Libraries
Spark has it's own libraries and third party packages which are built on it's design as a unified engine to provide unified API for common data analysis tasks