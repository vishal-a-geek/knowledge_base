#spark
To allow every executor to perform work in parallel, Spark breaks up the data into chunks, called partitions

A DataFrame's partition represents how data is physically distributed across cluster of machines during execution
- For one partition, Spark will have parallelism of only one executor , even if u have many other executors
- For many partitions, if there's only one executor, Spark uses parallelism of only one because there's only one computation resource.

DataFrames are high-level transformations, we need not manipulate partitions manually
For manual manipulation of partitions, Low Level APIs exist which allow achieving the same. ([[1. What is Apache Spark?#^1bf2c0|RDD]] interface)